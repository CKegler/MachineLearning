---
title: 'Course Project: Practical Machine Learning'
author: "Colin Kegler"
date: "Monday, November 16, 2015"
output: html_document
---
```{r, echo=FALSE, cache=TRUE }


```

## **Executive Summary**:
In their study, "Qualitative Activity Recognition of Weight Lifting Exercises", Velloso, E.; Bulling, A.; Gellersen, H., et al, collected bio-electronic sensor data on 6 subjects, using 5 pre-defined wieght-lifting exercises. Class A corresponds to the specified execution of the exercise, while the other 4 classes (B,C,D, and E) correspond to common mistakes. The dataset from these exercises has been made available to the general public. From it, we constructed a predictive model, based on PCA pre-processing and 10-fold KNN cross validation, to predict the class of exercise activity. 

## Exploratory Analysis

<ul>
<li>Discarding Items with Near Zero Variance</li>
    Columns with a highly repetitive or non-varying data values can cause a model to become unreliable, expecially during the validation phase. We removed 35 such values.
<pre>
> dim(pmlTrainingData)
[1] 19622   159
> dim(pmlTrainingData2)
[1] 19622   124
> 159-124
</pre>
    
<li>Remove columns with excessive number of NA values</li>
Several predictor columns primarily contained NA values (> 95% missing).  The large number of missing values would have made any attempt at imputing missing values from the sparse existing values, highly erroneous.

We removed 69 columns that were primarily NA values
<pre>
> dim(pmlTrainingData3)
[1] 19622    55
> dim(pmlTrainingData2)
[1] 19622   124
</pre?
<li>Explore linear combinations with other predictors</li> 
    Predictors that can be composed of linear combinations of other predictors add more variation to the model.  Fortunately, in this dataset no linear combinations of predictors were discovered.

     </pre>    
<li> Remove highly correlated descriptors </li>

    <pre>
    highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
    pmlTrainingData4 <- pmlTrainingData3[,-highlyCorDescr]
    </pre>
    The analysis found 22 highly correlated predictors above a absolute threshhold of 0.75, which were removed.
</li>
</ul>

## Training the Model 
--  Pre-processing with PCA (principal component analysis) 

Principal component analysis (PCA) was used to transform the data to a smaller sub-space where the new variable are uncorrelated with one another. The predictors are scaled to their mean and normalized by their standard deviation in the computation.

After the previous steps of pre-processing, our training model has been reduced to 33 predictors:

<pre>
> modelFit$coefnames
 [1] "raw_timestamp_part_1" "raw_timestamp_part_2" "num_window"           "pitch_belt"          
 [5] "yaw_belt"             "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"        
 [9] "magnet_belt_y"        "roll_arm"             "pitch_arm"            "yaw_arm"             
[13] "total_accel_arm"      "gyros_arm_y"          "gyros_arm_z"          "magnet_arm_x"        
[17] "magnet_arm_z"         "roll_dumbbell"        "pitch_dumbbell"       "yaw_dumbbell"        
[21] "total_accel_dumbbell" "gyros_dumbbell_y"     "roll_forearm"         "pitch_forearm"       
[25] "yaw_forearm"          "total_accel_forearm"  "gyros_forearm_x"      "gyros_forearm_z"     
[29] "accel_forearm_x"      "accel_forearm_z"      "magnet_forearm_x"     "magnet_forearm_y"    
[33] "magnet_forearm_z"  

</pre>

--  Cross-Validation with KNN (Nearest Neighbor)
The k-fold cross validation method involves splitting the dataset into k-subsets. For each subset is held out while the model is trained on all other subsets. This process is completed until accuracy is determined for each instance in the dataset; an overall accuracy estimate is provided.

We used 10-fold cross-validation with the model.

<pre>
> modelFit
k-Nearest Neighbors 

19622 samples
   33 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

Pre-processing: principal component signal extraction (33), centered (33), scaled (33) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 17661, 17658, 17660, 17659, 17659, 17660, ... 
Resampling results across tuning parameters:

  k  Accuracy   Kappa      Accuracy SD  Kappa SD   
  5  0.9675373  0.9589351  0.003337522  0.004222122
  7  0.9573958  0.9461076  0.004254488  0.005384654
  9  0.9458261  0.9314747  0.003321717  0.004199568

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was k = 5. 

</pre>

<b>Accuracy of the Predictors</b>

The 10-fold cross validation model on the training data yields a predictive accuracy of ~ 96.7% - the accuracy of prediction within the training data set. We do not aniticipate this accuracy to carry over as highly on the test data for the reasons:

(1) The sample size of the test data is rather small relative to the training data (i.e. n=20 observations in test compared to n=20,000+ observations for training data).  Each incorrect prediction of the model with the test data increases the error rate by 5%.

(2)  The use of knn validation may represent an over-fitting model of the training data which may not apply to the test data observations.  In essence, the model may accurately predict the training data, but its accuracy will be less accurately predcit the test data.

(3) We also expect less accuracy with the test data because we devised a model to predict the outcome, "classe", in the training data; no such column exists within the test data.  The absence of "classe" as an outcome variable in the test data means that we have to revise the model to predict an overlapping outcome, such as "raw_timestamp_part_1".  (Please refer to the <b>Addendum</b> section, where we observe that this time obervations fall into one of four distinct bins)

## Reproducibility
The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

```{r, echo=TRUE, cache=TRUE}

library(caret);
library(ggplot2);


naValues <- c("NA", "#DIV/0!")

# PART 1 (Data Input):read csv file
pmlTrainingData = read.csv("pml-training.csv", header=TRUE, na.strings=naValues, stringsAsFactors=FALSE, row.names=1)  

# PART 2 ( Basic Preprocessing of Data)
#Detect columns with near zero variance, as candidates to discard from training model
nsv <- nearZeroVar(pmlTrainingData,saveMetrics=TRUE)

pmlTrainingData2  <-   pmlTrainingData[, which(nsv$nzv==FALSE)]


# Remove columns with excessive number of NA values

pmlTrainingData3  <- pmlTrainingData2[, -which(colnames(pmlTrainingData2)=="classe")]
pmlTrainingData3  <- pmlTrainingData3[, -which(colnames(pmlTrainingData3)=="user_name")]
pmlTrainingData3  <- pmlTrainingData3[, -which(colnames(pmlTrainingData3)=="cvtd_timestamp")]
pmlTrainingData3  <- pmlTrainingData3[, -which(colSums(is.na(pmlTrainingData3))/nrow(pmlTrainingData3) > 0)]

# Find if some columns are linear combinations of other columns within data

comboInfo <- findLinearCombos(pmlTrainingData3)
# $linearCombos
# list()
# 
# $remove
# NULL

# We find that the pmlTrainingData3 object does not contain columns that are linear combinations of other columns


# Remove highly correlated descriptors with a threshhold above 0.75

descrCor <- cor(pmlTrainingData3)
summary(descrCor[upper.tri(descrCor)])

highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)

pmlTrainingData4 <- pmlTrainingData3[,-highlyCorDescr]

#PART 4:  Train model with PCA pre-processing and a k-nearest neighbor regression model
# USE PCA (Principal Component Analysis) to reduce number of covariates

# define training control ; use k-fold cross validation with 10 data slices
pmlTrainingData5 <- cbind(pmlTrainingData4,pmlTrainingData$classe)
names(pmlTrainingData5)[names(pmlTrainingData5) == 'pmlTrainingData$classe'] <- 'classe'

train_control <- trainControl(method="cv", number=10)

modelFit <- train(classe ~., data=pmlTrainingData5, method = "knn", preProcess=c("pca"), 
                 trControl = trainControl(method = "cv", number=10))

# make predictions from the training data
predictions <- predict(modelFit, pmlTrainingData)


## Part 5. Now use the Training Prediction Model, modelFit, on the TEST set


pmlTestingData = read.csv("pml-testing.csv", header=TRUE, na.strings=naValues, stringsAsFactors=FALSE, row.names=1)  

testPC <- predict(modelFit, pmlTestingData)


```


## Addendum


With the variable, "raw_timestamp_part_1", we observe that the times (which may be better interpreted as durations) fall into one of four distinct periods or bins.

```{r ,echo=FALSE, cache=TRUE}


naValues <- c("NA", "#DIV/0!")
pmlTrainingData = read.csv("pml-training.csv", header=TRUE, na.strings=naValues, stringsAsFactors=FALSE, row.names=1)  

# PART 2 ( Basic Preprocessing of Data)
#Detect columns with near zero variance, as candidates to discard from training model
nsv <- nearZeroVar(pmlTrainingData,saveMetrics=TRUE)

pmlTrainingData2  <-   pmlTrainingData[, which(nsv$nzv==FALSE)]

hist(pmlTrainingData2$raw_timestamp_part_1, labels=TRUE) 


```

